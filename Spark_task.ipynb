{"cells":[{"cell_type":"markdown","source":["### 1. How to Create a Data Frame – just a sample is fine."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3b6200cd-d7d5-45a2-82c5-9e91a313c8e3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cf24098b-89b4-4e39-ad61-1d15f3c657cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. How to Rename in the Dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"65f022e2-8841-4c5a-a6d0-8d751b8a7aa1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dict_cols = {'date': 'date_new', 'delay': 'delay_new'}\ndf_flights_new = df_flights.withColumnsRenamed(dict_cols) ### This function available in 3.40\ndf_flights_new.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8afc5acd-2780-4065-81b3-83448036bcdb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+---------+--------+------+-----------+\n|date_new|delay_new|distance|origin|destination|\n+--------+---------+--------+------+-----------+\n| 1011245|        6|     602|   ABE|        ATL|\n| 1020600|       -8|     369|   ABE|        DTW|\n| 1021245|       -2|     602|   ABE|        ATL|\n| 1020605|       -4|     602|   ABE|        ATL|\n| 1031245|       -4|     602|   ABE|        ATL|\n+--------+---------+--------+------+-----------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 3. What are the various types of Joins in a dataframe ?\n\nINNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"acf74703-867f-4ccf-b675-75a2c5f1b8d9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 4. What is the difference between Delta Live tables and data lake? Which one is better ?\n\nDelta Live Tables is Better. \n\n1. Delta live tables supports ACID Transactions\n2. Time Travel is supported\n3. No Partial Data found, if the job failed midway.\n4. Avoid too many small files.\n5. Schema Validation.\n6. Supports GDPR and Right to be Forgetten."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3c00a749-b9bd-41b1-9844-62073deaddc9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 5. How do you query data from Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da09295b-33dc-4c0e-ac71-e88f13658881","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from delta import *\ndf_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\ndf_flights.write.format(\"delta\").mode(\"append\").save(\"/deltafiles/flights_delta\")\ndf_delta = DeltaTable.forPath(spark,\"/deltafiles/flights_delta\" )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"588537ec-9bf2-4e6b-9753-08b7676dd70c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6. How to query data from Databricks , create PySpark job, how to run Pyspark\n\nUse read method to read data from files and load into a dataframe.Use write method to execute an action to create a pyspark job. Example given in question 5."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f6c59d1f-35f0-44cc-996f-e03619772729","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 7. Coding Assessment - Using a Slicer function in Python, write a program to check if a string is a palindrome or not"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"00c530af-b6e8-409a-937c-2f1d9eff35a3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["n = \"civic\"\nif str(n) == str(n)[::-1]:\n  print(\"It is a Palindrome\")\nelse:\n  print(\"Its not a Palindrome\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0d31e27d-3540-419c-a25a-ef66ec74e48d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["It is a Palindrome\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 8. We have a notebook, 3 samples CSV. Can you mount the files? How will you read the files and display the data using Databricks Community edition\n\nWe cannot mount the files from dbfs, Instead we can mount if the data is from ADLS, S3\n\nAllowed schemes are: gs,s3a,s3n,wasbs,adl,abfss."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"486ce6a7-7040-4629-9f2c-7e9b93d7cd23","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 9. Using Pyspark, how can you Loop through Rows in DataFrame Examples?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1e4f8640-4cbc-4f9c-8881-f713ce194878","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\ndf_flights.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"419ea10b-a58b-4178-95e2-a95da087642f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----+--------+------+-----------+\n|   date|delay|distance|origin|destination|\n+-------+-----+--------+------+-----------+\n|1011245|    6|     602|   ABE|        ATL|\n|1020600|   -8|     369|   ABE|        DTW|\n|1021245|   -2|     602|   ABE|        ATL|\n|1020605|   -4|     602|   ABE|        ATL|\n|1031245|   -4|     602|   ABE|        ATL|\n+-------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 10. How can you use Map() function to loop through rows in DataFrame?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"35f762c1-0b17-4456-8387-309e306ccb22","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\ncolumns = [\"date\",\"delay\",\"distance\",\"origin\",\"destination\"]\nrdd=df_flights.rdd.map(lambda x: \n    (x[\"date\"],x[\"delay\"],x[\"distance\"],x[\"origin\"],x[\"destination\"])\n    )  \nrdd.toDF(columns).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da686de4-b61a-4a3d-a8b6-28dd6ec863b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----+--------+------+-----------+\n|   date|delay|distance|origin|destination|\n+-------+-----+--------+------+-----------+\n|1011245|    6|     602|   ABE|        ATL|\n|1020600|   -8|     369|   ABE|        DTW|\n|1021245|   -2|     602|   ABE|        ATL|\n|1020605|   -4|     602|   ABE|        ATL|\n|1031245|   -4|     602|   ABE|        ATL|\n+-------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 11.How can you use foreach() to loop through rows in DataFrame?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"430fc0b8-3954-41c2-8afb-4fe952fe1bc0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def process_row(row):\n    date = row['date']\n    delay = row['delay']\n    origin = row['origin']\n    destination = row['destination']\n    print(f\"Processed row: {date}, {delay}, {origin}, {destination}\")\n\ndf_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\n\n# Apply the custom function to each row using foreach()\n# display(df_flights.foreach(lambda row: process_row(row)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"533e2109-c404-4e50-8332-fa185affda87","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 12. Using Pandas() to Iterate"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4adfe668-1f83-4630-8a24-0abb694d79ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\ndf_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\npandasDF = df_flights.toPandas()\n# for index, row in pandasDF.iterrows():\n#     # print(row['date'], row['delay'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"54767f2e-4089-4ea4-bcb1-79e4b9354387","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 13. We have a big collection of data , How can we use dataframes to Collect Data As List and Loop Through"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"59612d7f-c8b6-4f19-ace9-5a4cdfc9a413","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_flights = spark.read.format(\"csv\").option(\"header\", True).option(\"InferSchema\", True).load(\"/databricks-datasets/flights/departuredelays.csv\")\ndataout=df_flights.rdd.toLocalIterator()\n# for row in dataout:\n#     # print(row['date'],row['delay'])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a4e96fbd-e44a-4c5f-abf3-78e1dee00a94","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark_task","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2174089196614933,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
